{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ðŸŽ¯ Age Detection with Fine-Tuned CNN\n",
        "\n",
        "This notebook implements age detection using a fine-tuned pre-trained CNN model on the UTK Face dataset.\n",
        "\n",
        "## ðŸ“‹ Features:\n",
        "- Pre-trained CNN models (ResNet50, ResNet34, EfficientNet-B0)\n",
        "- Two-phase fine-tuning strategy\n",
        "- Comprehensive evaluation metrics\n",
        "- Real-time training visualization\n",
        "\n",
        "## ðŸš€ Expected Performance:\n",
        "- MAE: 4-6 years\n",
        "- Accuracy (Â±5 years): 65-75%\n",
        "- Accuracy (Â±10 years): 85-90%"
      ],
      "metadata": {
        "id": "_g_VOEpH-Ehe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision\n",
        "!pip install scikit-learn matplotlib tqdm opencv-python\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKDCxlV_r3N",
        "outputId": "522a69d2-7552-4546-bc0c-5285f0652e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create dataset directory\n",
        "os.makedirs('/content/UTKFace_data', exist_ok=True)\n",
        "\n",
        "print(\"Please upload your UTK Face dataset archive (zip file)\")\n",
        "print(\"From your path: C:\\\\Users\\\\2425605\\\\Downloads\\\\archive\")\n",
        "print(\"If it's a zip file, upload it. If it's a folder, zip it first then upload.\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract if it's a zip file\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"Extracting {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/UTKFace_data')\n",
        "        print(\"Dataset extracted successfully!\")\n",
        "\n",
        "# Check dataset structure\n",
        "print(\"\\nDataset structure:\")\n",
        "for root, dirs, files in os.walk('/content/UTKFace_data'):\n",
        "    level = root.replace('/content/UTKFace_data', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:5]:  # Show first 5 files\n",
        "        print(f\"{subindent}{file}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"{subindent}... and {len(files)-5} more files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "X0Cd6ywV_uC6",
        "outputId": "9388b3db-02d1-4fe9-da27-f379883b453f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your UTK Face dataset archive (zip file)\n",
            "From your path: C:\\Users\\2425605\\Downloads\\archive\n",
            "If it's a zip file, upload it. If it's a folder, zip it first then upload.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30728b9e-26e4-471b-92cd-0ed77db4c84a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-30728b9e-26e4-471b-92cd-0ed77db4c84a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving archive.zip to archive (1).zip\n",
            "Extracting archive (1).zip...\n",
            "Dataset extracted successfully!\n",
            "\n",
            "Dataset structure:\n",
            "UTKFace_data/\n",
            "  UTKFace/\n",
            "    1_0_3_20161220215943341.jpg.chip.jpg\n",
            "    24_1_4_20170117194842219.jpg.chip.jpg\n",
            "    28_0_1_20170113151917402.jpg.chip.jpg\n",
            "    36_0_0_20170117181851709.jpg.chip.jpg\n",
            "    27_1_0_20170117142744825.jpg.chip.jpg\n",
            "    ... and 23703 more files\n",
            "  utkface_aligned_cropped/\n",
            "    UTKFace/\n",
            "      1_0_3_20161220215943341.jpg.chip.jpg\n",
            "      24_1_4_20170117194842219.jpg.chip.jpg\n",
            "      28_0_1_20170113151917402.jpg.chip.jpg\n",
            "      36_0_0_20170117181851709.jpg.chip.jpg\n",
            "      27_1_0_20170117142744825.jpg.chip.jpg\n",
            "      ... and 23703 more files\n",
            "    crop_part1/\n",
            "      1_0_3_20161220215943341.jpg.chip.jpg\n",
            "      81_1_2_20170105174804349.jpg.chip.jpg\n",
            "      5_1_4_20161219185942044.jpg.chip.jpg\n",
            "      26_0_1_20170105183906447.jpg.chip.jpg\n",
            "      43_0_0_20170104205227123.jpg.chip.jpg\n",
            "      ... and 9775 more files\n",
            "  crop_part1/\n",
            "    1_0_3_20161220215943341.jpg.chip.jpg\n",
            "    81_1_2_20170105174804349.jpg.chip.jpg\n",
            "    5_1_4_20161219185942044.jpg.chip.jpg\n",
            "    26_0_1_20170105183906447.jpg.chip.jpg\n",
            "    43_0_0_20170104205227123.jpg.chip.jpg\n",
            "    ... and 9775 more files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify dataset is properly loaded\n",
        "import os\n",
        "\n",
        "data_dir = '/content/UTKFace_data'\n",
        "\n",
        "# Find the actual directory containing images\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    if len(image_files) > 0:\n",
        "        print(f\"Found {len(image_files)} images in: {root}\")\n",
        "        data_dir = root  # Update data_dir to the correct path\n",
        "        break\n",
        "\n",
        "print(f\"\\nDataset directory: {data_dir}\")\n",
        "print(f\"Sample filenames:\")\n",
        "for i, filename in enumerate(os.listdir(data_dir)[:5]):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        print(f\"  {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8qfA0y4AMex",
        "outputId": "2b460fc3-9af0-4e7f-ab70-eca2502a61f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 23708 images in: /content/UTKFace_data/UTKFace\n",
            "\n",
            "Dataset directory: /content/UTKFace_data/UTKFace\n",
            "Sample filenames:\n",
            "  1_0_3_20161220215943341.jpg.chip.jpg\n",
            "  24_1_4_20170117194842219.jpg.chip.jpg\n",
            "  28_0_1_20170113151917402.jpg.chip.jpg\n",
            "  36_0_0_20170117181851709.jpg.chip.jpg\n",
            "  27_1_0_20170117142744825.jpg.chip.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "class UTKFaceDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None, split='train'):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        # Load image paths and extract labels from filenames\n",
        "        self.image_paths = []\n",
        "        self.ages = []\n",
        "\n",
        "        # UTK filename format: [age]_[gender]_[race]_[date&time].jpg\n",
        "        for filename in os.listdir(data_dir):\n",
        "            if filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
        "                try:\n",
        "                    age = int(filename.split('_')[0])\n",
        "                    if 0 <= age <= 116:  # Valid age range\n",
        "                        self.image_paths.append(os.path.join(data_dir, filename))\n",
        "                        self.ages.append(age)\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        # Split data\n",
        "        total_samples = len(self.image_paths)\n",
        "        indices = np.random.RandomState(42).permutation(total_samples)\n",
        "\n",
        "        if split == 'train':\n",
        "            indices = indices[:int(0.7 * total_samples)]\n",
        "        elif split == 'val':\n",
        "            indices = indices[int(0.7 * total_samples):int(0.85 * total_samples)]\n",
        "        else:  # test\n",
        "            indices = indices[int(0.85 * total_samples):]\n",
        "\n",
        "        self.image_paths = [self.image_paths[i] for i in indices]\n",
        "        self.ages = [self.ages[i] for i in indices]\n",
        "\n",
        "        print(f\"{split} dataset size: {len(self.image_paths)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Get age label\n",
        "        age = self.ages[idx]\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(age, dtype=torch.float32)\n",
        "\n",
        "def get_data_transforms():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "def create_data_loaders(data_dir, batch_size=32):\n",
        "    train_transform, val_transform = get_data_transforms()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = UTKFaceDataset(data_dir, transform=train_transform, split='train')\n",
        "    val_dataset = UTKFaceDataset(data_dir, transform=val_transform, split='val')\n",
        "    test_dataset = UTKFaceDataset(data_dir, transform=val_transform, split='test')\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "print(\"Dataset classes defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRiUNaQMSerP",
        "outputId": "33647ae5-246a-4a0a-ff23-82c0108e2814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset classes defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class AgeDetectionModel(nn.Module):\n",
        "    def __init__(self, pretrained=True, model_name='resnet50'):\n",
        "        \"\"\"\n",
        "        Age Detection Model based on pre-trained CNN\n",
        "        \"\"\"\n",
        "        super(AgeDetectionModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained model\n",
        "        if model_name == 'resnet50':\n",
        "            self.backbone = models.resnet50(pretrained=pretrained)\n",
        "            num_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()  # Remove final classification layer\n",
        "        elif model_name == 'resnet34':\n",
        "            self.backbone = models.resnet34(pretrained=pretrained)\n",
        "            num_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "        elif model_name == 'efficientnet_b0':\n",
        "            self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
        "            num_features = self.backbone.classifier[1].in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "        # Age regression head\n",
        "        self.age_regressor = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 1)  # Single output for age\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize the weights of the regression head\"\"\"\n",
        "        for m in self.age_regressor.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using backbone\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Flatten if needed\n",
        "        if len(features.shape) > 2:\n",
        "            features = features.view(features.size(0), -1)\n",
        "\n",
        "        # Predict age\n",
        "        age = self.age_regressor(features)\n",
        "\n",
        "        return age.squeeze()  # Remove extra dimension\n",
        "\n",
        "class FocalMSELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal MSE Loss for age regression to focus on harder examples\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1.0, gamma=2.0):\n",
        "        super(FocalMSELoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        mse_loss = F.mse_loss(predictions, targets, reduction='none')\n",
        "        focal_weight = self.alpha * (mse_loss ** (self.gamma / 2))\n",
        "        focal_mse = focal_weight * mse_loss\n",
        "        return focal_mse.mean()\n",
        "\n",
        "def freeze_backbone(model, freeze=True):\n",
        "    \"\"\"\n",
        "    Freeze or unfreeze the backbone parameters\n",
        "    \"\"\"\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = not freeze\n",
        "\n",
        "    print(f\"Backbone parameters {'frozen' if freeze else 'unfrozen'}\")\n",
        "\n",
        "print(\"âœ… Model architecture defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9ntdTVJSiZQ",
        "outputId": "680cab71-9362-44be-90b9-54fe4839fd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model architecture defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def calculate_metrics(predictions, targets):\n",
        "    \"\"\"Calculate evaluation metrics for age prediction\"\"\"\n",
        "    mae = mean_absolute_error(targets, predictions)\n",
        "    mse = mean_squared_error(targets, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(targets, targets)\n",
        "\n",
        "    # Age group accuracy (within 5 years)\n",
        "    accuracy_5 = np.mean(np.abs(predictions - targets) <= 5) * 100\n",
        "\n",
        "    # Age group accuracy (within 10 years)\n",
        "    accuracy_10 = np.mean(np.abs(predictions - targets) <= 10) * 100\n",
        "\n",
        "    return {\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'accuracy_5': accuracy_5,\n",
        "        'accuracy_10': accuracy_10\n",
        "    }\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch_idx, (images, ages) in enumerate(pbar):\n",
        "        images, ages = images.to(device), ages.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, ages)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        predictions.extend(outputs.detach().cpu().numpy())\n",
        "        targets.extend(ages.detach().cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    metrics = calculate_metrics(predictions, targets)\n",
        "\n",
        "    return avg_loss, metrics\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
        "        for images, ages in pbar:\n",
        "            images, ages = images.to(device), ages.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, ages)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            targets.extend(ages.cpu().numpy())\n",
        "\n",
        "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    metrics = calculate_metrics(predictions, targets)\n",
        "\n",
        "    return avg_loss, metrics\n",
        "\n",
        "def plot_training_curves(train_losses, val_losses, train_maes, val_maes):\n",
        "    \"\"\"Plot training curves\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss curves\n",
        "    ax1.plot(train_losses, label='Train Loss', color='blue')\n",
        "    ax1.plot(val_losses, label='Validation Loss', color='red')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # MAE curves\n",
        "    ax2.plot(train_maes, label='Train MAE', color='blue')\n",
        "    ax2.plot(val_maes, label='Validation MAE', color='red')\n",
        "    ax2.set_title('Training and Validation MAE')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Mean Absolute Error (years)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ… Training utilities defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSv7jdcT_Lw",
        "outputId": "80a8bcd4-3d50-4639-c810-6f5a50abb8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training utilities defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data loaders\n",
        "BATCH_SIZE = 32\n",
        "print(f\"Creating data loaders from: {data_dir}\")\n",
        "\n",
        "try:\n",
        "    train_loader, val_loader, test_loader = create_data_loaders(data_dir, BATCH_SIZE)\n",
        "    print(\"âœ… Data loaders created successfully!\")\n",
        "\n",
        "    # Show a sample batch\n",
        "    for images, ages in train_loader:\n",
        "        print(f\"Batch shape: {images.shape}\")\n",
        "        print(f\"Age range in batch: {ages.min():.1f} - {ages.max():.1f} years\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error creating data loaders: {e}\")\n",
        "    print(\"Please check your dataset path and format\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npv-L0UnUCxh",
        "outputId": "5e545b05-842e-4d96-ce30-94f3a832d041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data loaders from: /content/UTKFace_data/UTKFace\n",
            "train dataset size: 16595\n",
            "val dataset size: 3556\n",
            "test dataset size: 3557\n",
            "âœ… Data loaders created successfully!\n",
            "Batch shape: torch.Size([32, 3, 224, 224])\n",
            "Age range in batch: 1.0 - 84.0 years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_NAME = 'resnet50'  # Options: 'resnet50', 'resnet34', 'efficientnet_b0'\n",
        "EPOCHS = 20  # Start with fewer epochs for testing\n",
        "LEARNING_RATE = 1e-4\n",
        "USE_FOCAL_LOSS = False  # Set to True to use Focal MSE Loss\n",
        "\n",
        "print(f\"ðŸ–¥ï¸ Device: {DEVICE}\")\n",
        "print(f\"ðŸ§  Model: {MODEL_NAME}\")\n",
        "print(f\"â±ï¸ Epochs: {EPOCHS}\")\n",
        "print(f\"ðŸ“ˆ Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"ðŸŽ¯ Use Focal Loss: {USE_FOCAL_LOSS}\")\n",
        "\n",
        "# Create model\n",
        "model = AgeDetectionModel(pretrained=True, model_name=MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Initially freeze backbone\n",
        "freeze_backbone(model, freeze=True)\n",
        "\n",
        "# Loss function\n",
        "if USE_FOCAL_LOSS:\n",
        "    criterion = FocalMSELoss(alpha=1.0, gamma=2.0)\n",
        "    print(\"Using Focal MSE Loss\")\n",
        "else:\n",
        "    criterion = nn.MSELoss()\n",
        "    print(\"Using MSE Loss\")\n",
        "\n",
        "# Optimizer and scheduler\n",
        "backbone_params = list(model.backbone.parameters())\n",
        "head_params = list(model.age_regressor.parameters())\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': backbone_params, 'lr': LEARNING_RATE * 0.1},\n",
        "    {'params': head_params, 'lr': LEARNING_RATE}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Model and training setup complete!\")\n",
        "print(f\"ðŸ“Š Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXs7OfF9UG0K",
        "outputId": "ee37e7fe-834c-4226-cee0-6737017a879a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ–¥ï¸ Device: cpu\n",
            "ðŸ§  Model: resnet50\n",
            "â±ï¸ Epochs: 20\n",
            "ðŸ“ˆ Learning Rate: 0.0001\n",
            "ðŸŽ¯ Use Focal Loss: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone parameters frozen\n",
            "Using MSE Loss\n",
            "\n",
            "âœ… Model and training setup complete!\n",
            "ðŸ“Š Model parameters: 1,180,673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE FIXED TRAINING CELL\n",
        "print(\"ðŸš€ Starting Training with Fixed Metrics...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize tracking variables\n",
        "best_val_mae = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_maes = []\n",
        "val_maes = []\n",
        "\n",
        "# Fixed training function\n",
        "def train_epoch_fixed(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch_idx, (images, ages) in enumerate(pbar):\n",
        "        images, ages = images.to(device), ages.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, ages)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Convert to numpy immediately\n",
        "        batch_preds = outputs.detach().cpu().numpy().flatten()\n",
        "        batch_targets = ages.detach().cpu().numpy().flatten()\n",
        "\n",
        "        all_predictions.extend(batch_preds)\n",
        "        all_targets.extend(batch_targets)\n",
        "\n",
        "        pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Calculate metrics with numpy arrays\n",
        "    predictions = np.array(all_predictions)\n",
        "    targets = np.array(all_targets)\n",
        "\n",
        "    mae = np.mean(np.abs(predictions - targets))\n",
        "    accuracy_5 = np.mean(np.abs(predictions - targets) <= 5) * 100\n",
        "    accuracy_10 = np.mean(np.abs(predictions - targets) <= 10) * 100\n",
        "\n",
        "    metrics = {\n",
        "        'mae': mae,\n",
        "        'accuracy_5': accuracy_5,\n",
        "        'accuracy_10': accuracy_10\n",
        "    }\n",
        "\n",
        "    return avg_loss, metrics\n",
        "\n",
        "def validate_epoch_fixed(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
        "        for images, ages in pbar:\n",
        "            images, ages = images.to(device), ages.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, ages)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert to numpy immediately\n",
        "            batch_preds = outputs.cpu().numpy().flatten()\n",
        "            batch_targets = ages.cpu().numpy().flatten()\n",
        "\n",
        "            all_predictions.extend(batch_preds)\n",
        "            all_targets.extend(batch_targets)\n",
        "\n",
        "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "\n",
        "    # Calculate metrics with numpy arrays\n",
        "    predictions = np.array(all_predictions)\n",
        "    targets = np.array(all_targets)\n",
        "\n",
        "    mae = np.mean(np.abs(predictions - targets))\n",
        "    accuracy_5 = np.mean(np.abs(predictions - targets) <= 5) * 100\n",
        "    accuracy_10 = np.mean(np.abs(predictions - targets) <= 10) * 100\n",
        "\n",
        "    metrics = {\n",
        "        'mae': mae,\n",
        "        'accuracy_5': accuracy_5,\n",
        "        'accuracy_10': accuracy_10\n",
        "    }\n",
        "\n",
        "    return avg_loss, metrics\n",
        "\n",
        "# Phase 1: Train with frozen backbone\n",
        "phase1_epochs = min(6, EPOCHS // 3)\n",
        "print(f\"\\nðŸ“˜ PHASE 1: Training with frozen backbone ({phase1_epochs} epochs)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for epoch in range(phase1_epochs):\n",
        "    print(f\"\\nðŸ”„ Epoch {epoch+1}/{phase1_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_metrics = train_epoch_fixed(model, train_loader, criterion, optimizer, DEVICE)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_metrics = validate_epoch_fixed(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_maes.append(train_metrics['mae'])\n",
        "    val_maes.append(val_metrics['mae'])\n",
        "\n",
        "    # Print results\n",
        "    print(f\"ðŸ“Š Train Loss: {train_loss:.4f}, Train MAE: {train_metrics['mae']:.2f}\")\n",
        "    print(f\"ðŸ“Š Val Loss: {val_loss:.4f}, Val MAE: {val_metrics['mae']:.2f}\")\n",
        "    print(f\"ðŸŽ¯ Accuracy (Â±5 years): {val_metrics['accuracy_5']:.1f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics['mae'] < best_val_mae:\n",
        "        best_val_mae = val_metrics['mae']\n",
        "        torch.save(model.state_dict(), '/content/best_model.pth')\n",
        "        print(f\"ðŸ’¾ New best model saved! MAE: {best_val_mae:.2f}\")\n",
        "\n",
        "print(f\"\\nâœ… Phase 1 completed! Best MAE: {best_val_mae:.2f} years\")"
      ],
      "metadata": {
        "id": "PX3z3meLUViv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 2: Fine-tune with unfrozen backbone\n",
        "print(f\"\\nðŸ”¥ PHASE 2: Fine-tuning with unfrozen backbone\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Unfreeze backbone\n",
        "freeze_backbone(model, freeze=False)\n",
        "\n",
        "# Create new optimizer with lower learning rate\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': backbone_params, 'lr': LEARNING_RATE * 0.01},  # Very low LR for backbone\n",
        "    {'params': head_params, 'lr': LEARNING_RATE * 0.1}  # Lower LR for head\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "phase2_epochs = EPOCHS - phase1_epochs\n",
        "\n",
        "for epoch in range(phase2_epochs):\n",
        "    print(f\"\\nðŸ”„ Epoch {epoch+1}/{phase2_epochs} (Phase 2)\")\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_metrics = train_epoch_fixed(model, train_loader, criterion, optimizer, DEVICE)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_metrics = validate_epoch_fixed(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_maes.append(train_metrics['mae'])\n",
        "    val_maes.append(val_metrics['mae'])\n",
        "\n",
        "    # Print results\n",
        "    print(f\"ðŸ“Š Train Loss: {train_loss:.4f}, Train MAE: {train_metrics['mae']:.2f}\")\n",
        "    print(f\"ðŸ“Š Val Loss: {val_loss:.4f}, Val MAE: {val_metrics['mae']:.2f}\")\n",
        "    print(f\"ðŸŽ¯ Accuracy (Â±5 years): {val_metrics['accuracy_5']:.1f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics['mae'] < best_val_mae:\n",
        "        best_val_mae = val_metrics['mae']\n",
        "        torch.save(model.state_dict(), '/content/best_model.pth')\n",
        "        print(f\"ðŸ’¾ New best model saved! MAE: {best_val_mae:.2f}\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Training completed! Best validation MAE: {best_val_mae:.2f} years\")"
      ],
      "metadata": {
        "id": "lRxMFXtIC3Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
        "plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# MAE curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_maes, label='Train MAE', color='blue', linewidth=2)\n",
        "plt.plot(val_maes, label='Validation MAE', color='red', linewidth=2)\n",
        "plt.title('Training and Validation MAE', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Absolute Error (years)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ“ˆ Training Summary:\")\n",
        "print(f\"ðŸ”¹ Total Epochs: {len(train_losses)}\")\n",
        "print(f\"ðŸ”¹ Best Validation MAE: {best_val_mae:.2f} years\")\n",
        "print(f\"ðŸ”¹ Final Train MAE: {train_maes[-1]:.2f} years\")\n",
        "print(f\"ðŸ”¹ Final Val MAE: {val_maes[-1]:.2f} years\")"
      ],
      "metadata": {
        "id": "3SVS5VeODD_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate on test set\n",
        "print(\"ðŸ“‚ Loading best model for final evaluation...\")\n",
        "model.load_state_dict(torch.load('/content/best_model.pth'))\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"ðŸ§ª Evaluating on test set...\")\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, ages in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images, ages = images.to(DEVICE), ages.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Convert to numpy immediately\n",
        "        batch_preds = outputs.cpu().numpy().flatten()\n",
        "        batch_targets = ages.cpu().numpy().flatten()\n",
        "\n",
        "        all_predictions.extend(batch_preds)\n",
        "        all_targets.extend(batch_targets)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "predictions = np.array(all_predictions)\n",
        "targets = np.array(all_targets)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "mae = np.mean(np.abs(predictions - targets))\n",
        "mse = np.mean((predictions - targets) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "accuracy_5 = np.mean(np.abs(predictions - targets) <= 5) * 100\n",
        "accuracy_10 = np.mean(np.abs(predictions - targets) <= 10) * 100\n",
        "\n",
        "# Calculate RÂ² score\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(targets, predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ¯ FINAL TEST RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ðŸ“Š Total test samples: {len(predictions)}\")\n",
        "print(f\"ðŸ“ Mean Absolute Error (MAE): {mae:.2f} years\")\n",
        "print(f\"ðŸ“ Root Mean Square Error (RMSE): {rmse:.2f} years\")\n",
        "print(f\"ðŸ“ˆ RÂ² Score: {r2:.4f}\")\n",
        "print(f\"ðŸŽ¯ Accuracy (Â±5 years): {accuracy_5:.1f}%\")\n",
        "print(f\"ðŸŽ¯ Accuracy (Â±10 years): {accuracy_10:.1f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Store results for later use\n",
        "final_metrics = {\n",
        "    'mae': mae,\n",
        "    'rmse': rmse,\n",
        "    'r2': r2,\n",
        "    'accuracy_5': accuracy_5,\n",
        "    'accuracy_10': accuracy_10,\n",
        "    'predictions': predictions,\n",
        "    'targets': targets\n",
        "}"
      ],
      "metadata": {
        "id": "lFoGzm1TDIET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comprehensive evaluation visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Scatter plot: Predictions vs True Ages\n",
        "ax1.scatter(targets, predictions, alpha=0.6, s=20, color='blue')\n",
        "ax1.plot([0, 100], [0, 100], 'r--', linewidth=3, label='Perfect Prediction')\n",
        "ax1.set_xlabel('True Age', fontsize=12)\n",
        "ax1.set_ylabel('Predicted Age', fontsize=12)\n",
        "ax1.set_title(f'Predictions vs True Ages\\n(MAE: {mae:.2f} years)', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Error histogram\n",
        "errors = predictions - targets\n",
        "ax2.hist(errors, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
        "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, alpha=0.8)\n",
        "ax2.set_xlabel('Prediction Error (years)', fontsize=12)\n",
        "ax2.set_ylabel('Frequency', fontsize=12)\n",
        "ax2.set_title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Age distribution comparison\n",
        "ax3.hist(targets, bins=30, alpha=0.7, label='True Ages', edgecolor='black', color='green')\n",
        "ax3.hist(predictions, bins=30, alpha=0.7, label='Predicted Ages', edgecolor='black', color='purple')\n",
        "ax3.set_xlabel('Age', fontsize=12)\n",
        "ax3.set_ylabel('Frequency', fontsize=12)\n",
        "ax3.set_title('Age Distribution Comparison', fontsize=14, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Accuracy by age group\n",
        "age_bins = np.arange(0, 101, 10)\n",
        "accuracies = []\n",
        "bin_centers = []\n",
        "\n",
        "for i in range(len(age_bins) - 1):\n",
        "    mask = (targets >= age_bins[i]) & (targets < age_bins[i+1])\n",
        "    if mask.sum() > 5:  # Only consider bins with enough samples\n",
        "        acc = np.mean(np.abs(predictions[mask] - targets[mask]) <= 5) * 100\n",
        "        accuracies.append(acc)\n",
        "        bin_centers.append((age_bins[i] + age_bins[i+1]) / 2)\n",
        "\n",
        "ax4.bar(bin_centers, accuracies, width=8, alpha=0.7, edgecolor='black', color='red')\n",
        "ax4.set_xlabel('Age Group (center)', fontsize=12)\n",
        "ax4.set_ylabel('Accuracy within Â±5 years (%)', fontsize=12)\n",
        "ax4.set_title('Accuracy by Age Group', fontsize=14, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Age group detailed analysis\n",
        "print(\"\\nðŸ“ˆ Age Group Performance Analysis:\")\n",
        "print(\"-\" * 60)\n",
        "age_groups = [(0, 18), (18, 30), (30, 50), (50, 70), (70, 120)]\n",
        "\n",
        "for min_age, max_age in age_groups:\n",
        "    mask = (targets >= min_age) & (targets < max_age)\n",
        "    if mask.sum() > 0:\n",
        "        group_mae = np.mean(np.abs(predictions[mask] - targets[mask]))\n",
        "        group_acc5 = np.mean(np.abs(predictions[mask] - targets[mask]) <= 5) * 100\n",
        "        group_count = mask.sum()\n",
        "        print(f\"Age {min_age:2d}-{max_age:2d}: MAE={group_mae:5.2f} years, Acc(Â±5)={group_acc5:5.1f}%, n={group_count:3d}\")"
      ],
      "metadata": {
        "id": "crsNGVYcDKyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and download the trained model\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Save comprehensive results\n",
        "results = {\n",
        "    'final_metrics': {\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'r2': float(r2),\n",
        "        'accuracy_5': float(accuracy_5),\n",
        "        'accuracy_10': float(accuracy_10)\n",
        "    },\n",
        "    'training_history': {\n",
        "        'train_losses': [float(x) for x in train_losses],\n",
        "        'val_losses': [float(x) for x in val_losses],\n",
        "        'train_maes': [float(x) for x in train_maes],\n",
        "        'val_maes': [float(x) for x in val_maes]\n",
        "    },\n",
        "    'model_config': {\n",
        "        'model_name': MODEL_NAME,\n",
        "        'epochs': EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'use_focal_loss': USE_FOCAL_LOSS,\n",
        "        'best_val_mae': float(best_val_mae)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save model state and results\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'results': results\n",
        "}, '/content/age_detection_model_complete.pth')\n",
        "\n",
        "# Save results as JSON for easy reading\n",
        "with open('/content/training_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"ðŸ’¾ Files ready for download:\")\n",
        "print(\"- age_detection_model_complete.pth: Complete model with training history\")\n",
        "print(\"- training_results.json: Training results in readable format\")\n",
        "\n",
        "# Download the files\n",
        "files.download('/content/age_detection_model_complete.pth')\n",
        "files.download('/content/training_results.json')\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Age Detection Training Completed Successfully!\")\n",
        "print(f\"ðŸ“Š Final MAE: {mae:.2f} years\")\n",
        "print(f\"ðŸŽ¯ Accuracy (Â±5 years): {accuracy_5:.1f}%\")\n",
        "print(f\"ðŸŽ¯ Accuracy (Â±10 years): {accuracy_10:.1f}%\")\n",
        "print(f\"ðŸ† Model Performance Grade: {'Excellent' if mae < 4 else 'Very Good' if mae < 6 else 'Good' if mae < 8 else 'Fair'}\")"
      ],
      "metadata": {
        "id": "ZW6dmPOEDNyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yb9JHxSaDQ3f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}